


# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV
from nltk.corpus import stopwords
import re
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.tokenize import TweetTokenizer


# Import the machine learning model of your choice
from sklearn.linear_model import LogisticRegression






# Download the dataset from Kaggle and specify the file path
df = pd.read_csv('../Data/training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)
df.head()


column_names = ['target', 'id', 'date-time', 'query_flag', 'twitter_handle', 'text']

# Add column headers to data frame
for i in df.columns:
    df[column_names[i]] = df[i]
    df.drop(columns=[i], inplace=True)

df





test_df = df[['text']].iloc[:5]
test_df


# Clean the text data, remove special characters, handle missing values, etc.
def process_text(doc):
    # Get rid of any retweet type characters along with standard stopwords corpus
    additional  = ['rt','rts','retweet']
    swords = set().union(stopwords.words('english'),additional)
    regex = re.compile("[^a-zA-Z ]")
    re_clean = regex.sub('', doc)
    lemmatizer = WordNetLemmatizer()
    lem = [lemmatizer.lemmatize(w) for w in word_tokenize(re_clean)]
    output = [word.lower() for word in lem if word.lower() not in swords]
    return ' '.join(output)


test_df['pre_process'] = test_df['text'].apply(lambda x: process_text(x))

test_df.head()





# Choose a feature extraction method (e.g., TF-IDF, Gensim, or a pretrained language model) and transform the text data into numerical features.
# tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')
# X = tfidf_vectorizer.fit_transform(df['text'])
# y = df['target']
# YOUR CODE HERE!





# Split the data into training and testing sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Choose a machine learning model (e.g., Logistic Regression) and train it
# model = LogisticRegression(max_iter=1000)
# model.fit(X_train, y_train)





# Perform sentiment analysis on the dataset using your trained model
# y_pred = model.predict(X_test)
# YOUR CODE HERE!





# Select an AI company or product of your choice and collect tweets related to it
# Use your trained model to predict sentiment on these tweets
# Create visualizations to showcase sentiment (e.g., bar charts, word clouds)

# Example: 
# - Visualize sentiment distribution using seaborn or matplotlib.
# - Create word clouds for positive and negative tweets.
# - Generate a bar chart showing sentiment scores for the chosen company/product.

# Additional Tips:
# - Experiment with hyperparameter tuning to improve model performance.
# - Use cross-validation for a more robust evaluation.
# - Write functions to encapsulate repetitive tasks and improve code organization.

# YOUR CODE HERE!





# Evaluate your model's performance using metrics like accuracy, precision, recall, and F1-score.

# Example:
# - accuracy = accuracy_score(y_test, y_pred)
# - classification_report(y_test, y_pred)
# - confusion_matrix(y_test, y_pred)

# Display your results and visualizations.

# YOUR CODE HERE!



